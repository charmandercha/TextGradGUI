import gradio as gr
import textgrad as tg
import requests
import os
import logging
from typing import Tuple, List, Dict, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√£o do Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "qwen3:4b" # Um bom modelo padr√£o para gera√ß√£o e cr√≠tica

# --- Fun√ß√µes de Conex√£o com Ollama (sem altera√ß√µes) ---
def validate_ollama_connection() -> Tuple[bool, List[str]]:
    """Valida conex√£o com Ollama e retorna modelos dispon√≠veis"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = sorted(list(set([model["name"] for model in models])))
            return True, model_names
        return False, []
    except requests.exceptions.RequestException:
        return False, []

def get_available_models() -> List[str]:
    """Recupera modelos dispon√≠veis no Ollama"""
    is_connected, models = validate_ollama_connection()
    return models if is_connected and models else [DEFAULT_MODEL, "qwen3", "llama3.1"]

# --- L√≥gica Principal de Otimiza√ß√£o (Refatorada) ---

def run_optimization_flow(
    question: str,
    optimization_criteria: str,
    iterations: int,
    model_name: str
) -> Tuple[str, str, List[Dict]]:
    """
    Executa um fluxo completo de gera√ß√£o e otimiza√ß√£o com TextGrad e Ollama.
    """
    # 1. Valida√ß√£o das Entradas do Usu√°rio
    if not question.strip() or len(question) < 10:
        error_msg = "A Pergunta √© obrigat√≥ria (m√≠nimo 10 caracteres)."
        history = [{"iteration": 0, "error": error_msg, "status": "validation_failed"}]
        return "Erro de valida√ß√£o", "Falha", history
    if not optimization_criteria.strip():
        error_msg = "Os Crit√©rios de Melhoria s√£o obrigat√≥rios."
        history = [{"iteration": 0, "error": error_msg, "status": "validation_failed"}]
        return "Erro de valida√ß√£o", "Falha", history

    try:
        # 2. Configurar o Engine TextGrad para usar Ollama via LiteLLM
        # NOTA: Usamos override=True para permitir que o engine seja reconfigurado
        # a cada execu√ß√£o sem causar erro.
        model_path = f"ollama/{model_name}"
        tg.set_backward_engine(f"experimental:{model_path}", override=True, cache=False)
        logger.info(f"Engine de otimiza√ß√£o configurado para o modelo: {model_name}")

        # 3. Definir Vari√°veis TextGrad
        question_var = tg.Variable(
            question,
            role_description="A pergunta do usu√°rio que precisa ser respondida.",
            requires_grad=False
        )

        # 4. GERA√á√ÉO DA RESPOSTA INICIAL (Novo passo crucial para UX)
        logger.info("Gerando a resposta inicial...")
        # Usamos BlackboxLLM para que o pr√≥prio modelo crie a primeira resposta.
        initial_model = tg.BlackboxLLM(f"experimental:{model_path}")
        answer_var = initial_model(question_var)
        # √â importante definir a descri√ß√£o do papel da vari√°vel que ser√° otimizada.
        answer_var.set_role_description("Uma resposta que ser√° otimizada iterativamente.")
        
        initial_answer_text = answer_var.value
        logger.info(f"Resposta inicial gerada: {initial_answer_text[:150]}...")

        history = [{
            "iteration": 0,
            "answer": initial_answer_text,
            "feedback": "Resposta inicial gerada pelo modelo.",
            "status": "initial"
        }]

        # 5. Configurar o Sistema de Otimiza√ß√£o
        # NOTA: Usando 'lr' pois sua vers√£o do textgrad espera este argumento.
        # Vers√µes mais novas podem esperar 'learning_rate'.
        optimizer = tg.TGD(parameters=[answer_var])
        loss_fn = tg.TextLoss(optimization_criteria)

        # 6. Loop de Otimiza√ß√£o
        logger.info(f"Iniciando {iterations} itera√ß√µes de otimiza√ß√£o...")
        for i in range(iterations):
            try:
                logger.info(f"Executando itera√ß√£o {i + 1}/{iterations}")
                
                # Calcula a "perda" (o feedback textual)
                loss = loss_fn(answer_var)
                
                # Calcula o "gradiente" (a instru√ß√£o de melhoria)
                loss.backward()
                
                # Aplica a otimiza√ß√£o
                optimizer.step()
                
                # Limpa o gradiente para a pr√≥xima itera√ß√£o
                optimizer.zero_grad()
                
                # Salva o progresso
                history.append({
                    "iteration": i + 1,
                    "answer": answer_var.value,
                    "feedback": loss.value, # O feedback √© o valor da "loss"
                    "status": "success"
                })
                logger.info(f"Itera√ß√£o {i + 1} conclu√≠da.")

            except Exception as step_error:
                error_msg = f"Erro na itera√ß√£o {i + 1}: {step_error}"
                logger.error(error_msg)
                history.append({"iteration": i + 1, "error": error_msg, "status": "failed"})
                # Para o processo se uma itera√ß√£o falhar
                break

        final_answer = history[-1].get("answer", initial_answer_text)
        final_status = f"Conclu√≠do ap√≥s {len(history) - 1} itera√ß√µes."
        return final_answer, final_status, history

    except Exception as e:
        error_msg = f"Erro geral no sistema: {e}"
        logger.error(error_msg, exc_info=True)
        history = [{"iteration": 0, "error": error_msg, "status": "system_error"}]
        return "Ocorreu um erro no sistema.", "Falha", history


# --- Interface Gr√°fica com Gradio (Refatorada para melhor UX) ---

with gr.Blocks(title="TextGrad Optimizer com Ollama", theme=gr.themes.Soft()) as interface:
    gr.Markdown("""
    # üöÄ Otimizador de Textos com TextGrad & Ollama
    Escreva uma pergunta e defina como a resposta deve ser melhorada. A IA ir√° gerar uma resposta inicial e depois a otimizar√° iterativamente seguindo seus crit√©rios.
    """)

    with gr.Row():
        connection_status = gr.HTML()

    with gr.Row(variant="panel"):
        with gr.Column(scale=2):
            gr.Markdown("### 1. Defina a Tarefa")
            
            question_input = gr.Textbox(
                label="Pergunta ou Instru√ß√£o",
                placeholder="Ex: Explique o que √© a Teoria da Relatividade.",
                lines=4
            )

            optimization_criteria = gr.Textbox(
                label="Crit√©rios para Melhoria (Loss Function)",
                info="Como a resposta deve ser avaliada e melhorada a cada passo?",
                value="Seja muito cr√≠tico. Avalie a clareza, precis√£o e simplicidade da resposta. Aponte erros factuais e sugira formas de tornar a explica√ß√£o mais f√°cil de entender.",
                lines=5
            )

            gr.Markdown("### 2. Configure a Otimiza√ß√£o")
            
            model_dropdown = gr.Dropdown(
                choices=get_available_models(),
                value=DEFAULT_MODEL if DEFAULT_MODEL in get_available_models() else (get_available_models()[0] if get_available_models() else ""),
                label="Modelo Ollama",
                info="Modelo usado para gerar e otimizar a resposta."
            )
            
            iterations = gr.Slider(
                1, 5, value=2, step=1,
                label="Ciclos de Otimiza√ß√£o",
                info="Quantas vezes a IA deve tentar melhorar a pr√≥pria resposta."
            )
            
            with gr.Row():
                optimize_btn = gr.Button("üß† Gerar e Otimizar", variant="primary", scale=3)
                clear_btn = gr.Button("üóëÔ∏è Limpar", scale=1)

        with gr.Column(scale=3):
            gr.Markdown("### 3. Acompanhe o Resultado")
            
            with gr.Tab("‚úÖ Resposta Final"):
                optimized_answer = gr.Textbox(
                    label="Resposta Otimizada",
                    lines=15,
                    interactive=False,
                    show_copy_button=True
                )
                final_status = gr.Textbox(label="Status Final", interactive=False)
                
            with gr.Tab("üìú Hist√≥rico Detalhado"):
                history_display = gr.JSON(label="Log de Itera√ß√µes")

    # --- L√≥gica de Eventos da Interface ---

    def check_connection_status():
        is_connected, models = validate_ollama_connection()
        if is_connected:
            return f"<p style='color:green;'>‚úÖ Ollama conectado | Modelos dispon√≠veis: {len(models)}</p>"
        return f"<p style='color:red;'>‚ùå Ollama n√£o detectado em {OLLAMA_BASE_URL}. Verifique se est√° em execu√ß√£o.</p>"

    # Ao carregar a interface, verifica a conex√£o
    interface.load(fn=check_connection_status, outputs=[connection_status])
    
    # A√ß√£o do bot√£o de otimizar
    optimize_btn.click(
        fn=run_optimization_flow,
        inputs=[question_input, optimization_criteria, iterations, model_dropdown],
        outputs=[optimized_answer, final_status, history_display]
    )
    
    # A√ß√£o do bot√£o de limpar
    clear_btn.click(
        fn=lambda: ("", "Avalie a clareza, precis√£o e simplicidade...", 2, "", "Aguardando otimiza√ß√£o...", None),
        outputs=[question_input, optimization_criteria, iterations, optimized_answer, final_status, history_display]
    )

if __name__ == "__main__":
    is_connected, models = validate_ollama_connection()
    if is_connected:
        print("‚úÖ Ollama detectado!")
        print(f"üì¶ Modelos dispon√≠veis: {models}")
        print("üöÄ Iniciando TextGrad Optimizer...")
        interface.launch(server_name="127.0.0.1", server_port=7860)
    else:
        print("‚ùå ERRO: Ollama n√£o foi detectado ou n√£o est√° acess√≠vel.")
        print(f"   Por favor, garanta que o Ollama esteja em execu√ß√£o e acess√≠vel em '{OLLAMA_BASE_URL}'.")